{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with Word Embeddings, Tensorflow, and Keras\n",
    "\n",
    "- We'll be using the [pymagnitude](https://github.com/plasticityai/magnitude) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymagnitude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You'll need to download embedding 'model' files manually\n",
    "\n",
    "Start by downloading one of the following:\n",
    "\n",
    "- [GloVe](http://magnitude.plasticity.ai/glove/medium/glove.6B.300d.magnitude)\n",
    "- [word2vec](http://magnitude.plasticity.ai/word2vec/heavy/GoogleNews-vectors-negative300.magnitude)\n",
    "- [fastText](http://magnitude.plasticity.ai/fasttext/light/wiki-news-300d-1M.magnitude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to the name/path of the embedding file you donwloaded\n",
    "path = 'glove.6B.300d.magnitude'\n",
    "\n",
    "vectors = Magnitude(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.dim # this is how big the vectors are for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cat\" in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working [-0.0332886  0.0680554 -0.0059854]\n",
      "community [-0.0431237 -0.0621079  0.0222967]\n",
      "eight [-0.0645148  0.0757905 -0.0573475]\n",
      "groups [-0.0194742  0.0416667 -0.0084207]\n",
      "despite [ 0.0479242  0.02233   -0.0047541]\n",
      "level [-0.0848005  0.1553303  0.0087196]\n",
      "largest [-0.0233524  0.04329   -0.0260668]\n",
      "whose [-0.0069965  0.0276343  0.0280235]\n",
      "attacks [0.03818   0.0117073 0.0706972]\n",
      "germany [ 0.0062351  0.0252957 -0.0618449]\n"
     ]
    }
   ],
   "source": [
    "for key, vector in vectors[500:510]:\n",
    "    print(key, vector[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0463976,  0.0525527, -0.007488 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.query(\"cat\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0463976,  0.0525527, -0.007488 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.query([\"cat\",\"dog\"])[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7979039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.distance(\"cat\", \"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3062327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.distance(\"cat\", \"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar_to_given(\"cat\", [\"dog\", \"television\", \"laptop\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.doesnt_match([\"breakfast\", \"cereal\", \"dinner\", \"lunch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.6816746),\n",
       " ('cats', 0.68158376),\n",
       " ('pet', 0.5870366),\n",
       " ('dogs', 0.5407667),\n",
       " ('feline', 0.489797)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar(\"cat\", topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713276),\n",
       " ('princess', 0.5432625),\n",
       " ('throne', 0.53861046),\n",
       " ('monarch', 0.53475744),\n",
       " ('daughter', 0.49802512),\n",
       " ('mother', 0.49564433),\n",
       " ('elizabeth', 0.48326522),\n",
       " ('kingdom', 0.47747076),\n",
       " ('prince', 0.46682397),\n",
       " ('wife', 0.4647327)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.most_similar(positive = [\"woman\", \"king\"], negative = [\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "- Given a document, determine the topic of the document\n",
    "- For this task, we'll use the Brown corpus of texts accessible via NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-ae9ed7ce8d6f>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for fileid in tqdm(brown.fileids(categories=[cat])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1cb0fcf277417ca6ac51a66b9b8026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belles_lettres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f509f4443945aa8d293aedd58092de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "editorial\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c142bc0010f4022915580e8049e2fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b538700c51d640519643ba6d32a0a514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd79fab9be004030af3aea336bf37245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hobbies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafeeb3cbca044ea999666ad54c7cf41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1904d8eace64569baecea3785db2054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a94bd3e9f7498e9acf6c37e7418a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lore\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278f913b71ae465ea924d0370be041f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mystery\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09148efe7ba8489a84e5dd632411f4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c2986aa1a34a74831066e499a11754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "religion\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9796ad015a7645aa9eff13b241bb8eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0242c5f1157f43fe8480f6f160e6a86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c8970a77ec46f899dca44e4ed526dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science_fiction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12ac7d42e0f4749bccb883af0fb0902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "import tqdm # tqdm displays a progress bar\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "category_vectors = []\n",
    "\n",
    "cats = brown.categories()\n",
    "    \n",
    "# for each category\n",
    "for cat in cats:\n",
    "    print(cat)\n",
    "    # grab all of the documents\n",
    "    for fileid in tqdm(brown.fileids(categories=[cat])):\n",
    "        words = list(map(str.lower, brown.words(fileids=[fileid])))\n",
    "        # grab all of the words, find their embedding, sum all embeddings\n",
    "        word_sum = np.sum([vectors.query([w]) for w in words if w in vectors], axis=0) # why axis=0?\n",
    "        # add the now summed embedding to the list for this category\n",
    "        category_vectors.append((cat,word_sum))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "keys,values=zip(*category_vectors) # unzip using a *\n",
    "\n",
    "data = pd.DataFrame({'cat':keys,'vectors':values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventure</td>\n",
       "      <td>[[-62.71034, 23.958982, -9.739936, -54.190174,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventure</td>\n",
       "      <td>[[-54.73519, 19.858883, -8.82027, -59.00295, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventure</td>\n",
       "      <td>[[-46.095287, 24.262121, -7.475177, -59.681107...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat                                            vectors\n",
       "0  adventure  [[-62.71034, 23.958982, -9.739936, -54.190174,...\n",
       "1  adventure  [[-54.73519, 19.858883, -8.82027, -59.00295, -...\n",
       "2  adventure  [[-46.095287, 24.262121, -7.475177, -59.681107..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(data)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random baseline 0.06666666666666667\n",
      "most common baseline?\n",
      "adventure 0.058\n",
      "belles_lettres 0.15\n",
      "editorial 0.054\n",
      "fiction 0.058\n",
      "government 0.06\n",
      "hobbies 0.072\n",
      "humor 0.018\n",
      "learned 0.16\n",
      "lore 0.096\n",
      "mystery 0.048\n",
      "news 0.088\n",
      "religion 0.034\n",
      "reviews 0.034\n",
      "romance 0.058\n",
      "science_fiction 0.012\n"
     ]
    }
   ],
   "source": [
    "print('random baseline {}'.format(1.0/len(cat)))\n",
    "\n",
    "print('most common baseline?')\n",
    "for cat in cats:\n",
    "    print(cat, len(data[data.cat==cat])/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.cat==cat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 2), (450, 2))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data.sample(frac=0.1,random_state=200)\n",
    "train = data.drop(test.index)\n",
    "\n",
    "test.shape, train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data.cat) \n",
    "X = [x[0] for x in train.vectors]\n",
    "y = le.transform(train.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfr = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alami\\Anaconda3\\envs\\newenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "test = data.sample(frac=0.1,random_state=200)\n",
    "train = data.drop(test.index)\n",
    "\n",
    "le = preprocessing.LabelEncoder() # convert to numerical categories\n",
    "ohe = preprocessing.OneHotEncoder() # convert categories to distributions (i.e., 1-hot vectors)\n",
    "le.fit(data.cat) \n",
    "y = le.transform(train.cat).reshape(-1, 1) # this is magic\n",
    "ohe.fit(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((450, 300), (450, 15))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ohe.transform(y).todense()\n",
    "\n",
    "X = np.array([x[0] for x in train.vectors])\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split of model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs =keras.Input(shape=(300), name='ani_image')\n",
    "x = layers.Flatten(name = 'flattened_img')(inputs)\n",
    "x=layers.Dense(1024,activation='relu', input_shape=(151,))(x) \n",
    "x=layers.Dense(1024,activation='relu')(x) \n",
    "x=layers.Dense(512,activation='relu')(x) \n",
    "preds=Dense(1,activation='softmax')(x) \n",
    "model=Model(inputs=inputs,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=300, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15)                15375     \n",
      "=================================================================\n",
      "Total params: 1,373,199\n",
      "Trainable params: 1,373,199\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To visualize neural network\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5934 - accuracy: 0.78 - ETA: 0s - loss: 0.6387 - accuracy: 0.77 - 0s 41ms/step - loss: 0.6252 - accuracy: 0.7753 - val_loss: 2.2362 - val_accuracy: 0.4444\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.82 - ETA: 0s - loss: 0.6406 - accuracy: 0.79 - 0s 30ms/step - loss: 0.6357 - accuracy: 0.7827 - val_loss: 2.0654 - val_accuracy: 0.4222\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.76 - ETA: 0s - loss: 0.6044 - accuracy: 0.77 - 0s 28ms/step - loss: 0.6117 - accuracy: 0.7827 - val_loss: 2.0005 - val_accuracy: 0.4222\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5149 - accuracy: 0.87 - ETA: 0s - loss: 0.5520 - accuracy: 0.80 - 0s 29ms/step - loss: 0.5489 - accuracy: 0.8148 - val_loss: 2.0336 - val_accuracy: 0.4889\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.89 - ETA: 0s - loss: 0.5442 - accuracy: 0.83 - 0s 28ms/step - loss: 0.5381 - accuracy: 0.8395 - val_loss: 1.9847 - val_accuracy: 0.4889\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.82 - ETA: 0s - loss: 0.5728 - accuracy: 0.80 - 0s 31ms/step - loss: 0.5555 - accuracy: 0.8148 - val_loss: 2.1609 - val_accuracy: 0.4000\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4560 - accuracy: 0.85 - ETA: 0s - loss: 0.4889 - accuracy: 0.83 - 0s 29ms/step - loss: 0.4986 - accuracy: 0.8420 - val_loss: 1.9553 - val_accuracy: 0.4889\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3741 - accuracy: 0.89 - ETA: 0s - loss: 0.3939 - accuracy: 0.89 - 0s 31ms/step - loss: 0.4061 - accuracy: 0.8889 - val_loss: 2.0570 - val_accuracy: 0.4222\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.81 - ETA: 0s - loss: 0.3908 - accuracy: 0.85 - 0s 31ms/step - loss: 0.3858 - accuracy: 0.8691 - val_loss: 2.0780 - val_accuracy: 0.5556\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.76 - ETA: 0s - loss: 0.4555 - accuracy: 0.82 - 0s 29ms/step - loss: 0.4917 - accuracy: 0.8099 - val_loss: 2.1324 - val_accuracy: 0.4667\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4153 - accuracy: 0.87 - ETA: 0s - loss: 0.4157 - accuracy: 0.84 - 0s 32ms/step - loss: 0.4314 - accuracy: 0.8420 - val_loss: 2.0628 - val_accuracy: 0.4667\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.87 - ETA: 0s - loss: 0.3582 - accuracy: 0.88 - ETA: 0s - loss: 0.3839 - accuracy: 0.88 - 0s 37ms/step - loss: 0.3675 - accuracy: 0.8914 - val_loss: 2.1283 - val_accuracy: 0.4667\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.89 - ETA: 0s - loss: 0.3124 - accuracy: 0.91 - 0s 33ms/step - loss: 0.3214 - accuracy: 0.9185 - val_loss: 2.1991 - val_accuracy: 0.4222\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4809 - accuracy: 0.82 - ETA: 0s - loss: 0.4468 - accuracy: 0.85 - 0s 33ms/step - loss: 0.4144 - accuracy: 0.8716 - val_loss: 2.2007 - val_accuracy: 0.4667\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.78 - ETA: 0s - loss: 0.5322 - accuracy: 0.83 - ETA: 0s - loss: 0.4838 - accuracy: 0.85 - 0s 39ms/step - loss: 0.4784 - accuracy: 0.8543 - val_loss: 2.0790 - val_accuracy: 0.4444\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.82 - ETA: 0s - loss: 0.3146 - accuracy: 0.90 - 0s 33ms/step - loss: 0.3211 - accuracy: 0.9012 - val_loss: 2.0860 - val_accuracy: 0.4889\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.92 - ETA: 0s - loss: 0.2759 - accuracy: 0.92 - 0s 32ms/step - loss: 0.2955 - accuracy: 0.9136 - val_loss: 2.1594 - val_accuracy: 0.4667\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.89 - ETA: 0s - loss: 0.3227 - accuracy: 0.89 - 0s 32ms/step - loss: 0.3081 - accuracy: 0.9037 - val_loss: 2.2278 - val_accuracy: 0.4444\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.92 - ETA: 0s - loss: 0.2767 - accuracy: 0.92 - 0s 30ms/step - loss: 0.2673 - accuracy: 0.9309 - val_loss: 2.1885 - val_accuracy: 0.5111\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2999 - accuracy: 0.93 - ETA: 0s - loss: 0.2275 - accuracy: 0.96 - 0s 29ms/step - loss: 0.2183 - accuracy: 0.9679 - val_loss: 2.1998 - val_accuracy: 0.4889\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test,y_test), epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 48.888888888888886\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "#Converting predictions to label\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "\n",
    "#Converting one hot encoded test label to label\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(pred,test)\n",
    "print('Accuracy is:', a*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.Model(inputs=X, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence)\n",
    "#model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss=tf.keras.losses.KLDivergence(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X,y,batch_size=64,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.Model.fit(X,y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = le.transform(test.cat)\n",
    "test_X = [x[0] for x in test.vectors]\n",
    "\n",
    "score = accuracy_score(clfr.predict(test_X), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.300d.magnitude 0.52\n"
     ]
    }
   ],
   "source": [
    "print(path, score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data/GoogleNews-vectors-negative300.magnitude 0.4\n",
    "data/wiki-news-300d-1M.magnitude 0.56\n",
    "data/glove.6B.300d.magnitude 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What would you say is the neural network \"learning\"?\n",
    "- learning the most common topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the depth or width of the network affect the training and the results?\n",
    "- The more the width and the depth the more accurate the results will be. First I used only 64, 32 width and 3 layers, at that time it had 60% accuracy. When I changed width 1024 , it became above 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you made changes to the network, what do you notice about how parameters (network depth, number of nodes, learning rate, etc.) and how they interact with each other? We said that neural networks are learning non-convex problems, but what about finding the best parameters? Is that a convex problem?\n",
    "- Finding the best parameter for one layer could be a convex problem, but for more than one layer, it may not be a convex problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is regularization? Why is it important?\n",
    "- Regularization is discouraging the train data to learn more complex or flexible model which involves a loss function where the coefficients are chosen, such that they minimize this loss function.It is important because it avoids overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which activation functions did you choose (besides logitistic/sigmoid)? For one of the activation functions you tried, spend some time learning about it. Whereas logistic/sigmoid maps from inputs to a probability between 0-1, what does the activation function you chose do?\n",
    "- In my model.sequential function the activation function is Relu. This function returns the highest value between 0 and x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- I couldn't set up my environment or intall pymagnitude. At last, using Jake Carns's trello comment I set up another environment and install pymagnitude. \n",
    "- Thus, I am turning it when the environment is set up.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install okpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Assignment: A5 Topic Modeling with MLPs\n",
      "OK, version v1.18.1\n",
      "=====================================================================\n",
      "\n",
      "Successfully logged in as SajiaZafreen@u.boisestate.edu\n"
     ]
    }
   ],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('a5.ok')\n",
    "ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving notebook... Saved 'A5-topic-modeling.ipynb'.\n",
      "Submit... 0.0% complete\n",
      "Could not submit: Late Submission of bsu/nlp/sp21/a5\n",
      "Backup... 100% complete\n",
      "Backup past deadline by 5 days, 14 hours, 24 minutes, and 6 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
